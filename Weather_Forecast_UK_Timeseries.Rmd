#Loading library
install.packages("zoo")
install.packages("dplyr")
install.packages("forecast")
install.packages("ggplot2")
install.packages("seasonal")
install.packages("reshape2")
install.packages("tidyr")
install.packages("GGally")
install.packages("randomForest")
install.packages("rpart")
install.packages("e1071")
install.packages("caret")
install.packages("psych")
install.packages("xts")
library(zoo)
library(ggplot2)
library(dplyr)
library(forecast)
library(ggplot2)
library(seasonal)
library(reshape2)
library(tidyr)
library(GGally)
library(randomForest)
library(rpart)
library(e1071)
library(caret)
library(psych)
library(xts)



# Read the CSV file
data <- read.csv("data.csv", header = FALSE, stringsAsFactors = FALSE)

# Retain rows 1, 2, and 2659 retaining top 2 rows and Latitude longitude for City: Glasgow
retained_rows <- data[c(1, 2, 2659), ]

# View the retained rows
print(retained_rows)
View(retained_rows)

df <- retained_rows

retained_cols <- df[,-c(1,2)]
View(retained_cols)

final_data <- retained_cols

# Extract date_time values from the first row
date_times <- unlist(final_data[1, seq(1, ncol(final_data), by = 10)])

# Initialize an empty data frame to store the result
result <- data.frame(date_time = character(), TSK = numeric(), PSFC = numeric(), U10 = numeric(), V10 = numeric(), Q2 = numeric(), RAINC = numeric(), RAINNC = numeric(), SNOW = numeric(), TSLB = numeric(), SMOIS = numeric(), stringsAsFactors = FALSE)

# Loop over the columns, starting from column 1 and increment by 10
for (i in seq(1, length(date_times))) {
  # Extract values from the third row
  values <- unlist(final_data[3, seq((i - 1) * 10 + 1, (i - 1) * 10 + 10)])
  
  # Combine date_time values with column names and values
  temp <- data.frame(date_time = rep(date_times[i], 1), TSK = values[1], PSFC = values[2], U10 = values[3], V10 = values[4], Q2 = values[5], RAINC = values[6], RAINNC = values[7], SNOW = values[8], TSLB = values[9], SMOIS = values[10], stringsAsFactors = FALSE)
  
  # Append the temporary data frame to the result
  result <- rbind(result, temp)
}

# Print the result
print(result)
View(result)

# Check the number of rows in the dataframe 'result'
num_rows <- nrow(result)

# Print the number of rows
print(num_rows)

# Remove the 'X' character from the date_time column
result$date_time <- sub("^X", "", result$date_time)

# Check for missing values and count in each column
missing_count <- colSums(is.na(result))

# Print the missing count in each column
print(missing_count)


# Print the updated dataframe
glimpse(result)
View(result)


# Convert the date and time column into POSIXct format
result$date_time <- as.POSIXct(result$date_time, format = "%d.%m.%Y.%H.%M")

# Print the structure of the data frame to confirm the conversion
str(result)
View(result)
# Specify the columns to perform the replacement
columns_to_replace <- c("TSK", "PSFC", "U10", "V10", "Q2", "RAINC", "RAINNC", "SNOW", "TSLB", "SMOIS")

# Loop over the columns
for (col in columns_to_replace) {
  # Convert the column to numeric
  result[[col]] <- as.numeric(as.character(result[[col]]))
  
  # Find indices of missing values in the column
  missing_values <- which(is.na(result[[col]]))
  
  for (i in missing_values) {    if (i == 1) {      
  # If the missing value is in the first row, replace it with the next observed value      
    result[[col]][i] <- na.locf(result[[col]])[i]    
    } 
    else if (i < length(result[[col]])) 
      {      
      # Find the indices of the previous and next non-NA values      
      prev_value <- max(which(!is.na(result[[col]]))[which(!is.na(result[[col]])) < i])      
      next_value <- min(which(!is.na(result[[col]]))[which(!is.na(result[[col]])) > i])            
      # If both previous and next values are NA, skip the replacement      
      if (is.na(prev_value) && is.na(next_value)) next            
      # If only one of the adjacent values is NA, replace NA value with the available value      
      if (is.na(prev_value)) {        
      result[[col]][i] <- result[[col]][next_value]      
      } 
      else if (is.na(next_value)) 
      {        
       result[[col]][i] <- result[[col]][previous_value]      
      } 
      else {        
        # Calculate the average of previous and next non-NA values and replace NA value        
        result[[col]][i] <- (result[[col]][prev_value] + result[[col]][next_value]) / 2      
      }    
    }  
  }
}

# Print the updated dataframe
View(result)

# Check for missing values and count in each column
missing_count <- colSums(is.na(result))

# Print the missing count in each column
print(missing_count)

# Replace missing value in the date_time column of the last row
result[nrow(result), "date_time"] <- "2018-05-31 21:00:00"


View(result)

# Identify the columns to convert to numeric (excluding the date_time column)
numeric_cols <- setdiff(names(result), "date_time")

# Convert the identified columns to numeric
result[numeric_cols] <- lapply(result[numeric_cols], as.numeric)

# Check the structure of the dataframe
str(result)
sapply(result, class)
summary(result)

# Function to detect outliers using IQR
# Loop over each column
for (col in names(result)) {
  # Calculate the lower and upper quartiles
  q1 <- quantile(result[[col]], 0.25)
  q3 <- quantile(result[[col]], 0.75)
  
  # Calculate the interquartile range (IQR)
  iqr <- q3 - q1
  
  # Calculate the lower and upper bounds for outliers detection
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  
  # Detect outliers
  outliers <- result[[col]] < lower_bound | result[[col]] > upper_bound
  
  # Print the row numbers and column name where outliers are detected
  if (any(outliers)) {
    print(paste("Column:", col))
    print(which(outliers))
  }
}

#ignoring the outliers as the weather can be extreme in Glasgow, Scotland


install.packages("corrplot")
library(corrplot)

# Correlation Analysis
correlation_matrix <- cor(result[c("TSK", "PSFC", "U10", "V10", "Q2", "RAINC", "RAINNC", "SNOW", "TSLB", "SMOIS")], method="pearson" )
print(correlation_matrix)
# Plot correlation matrix
corrplot(correlation_matrix, method = "circle")
corrplot(correlation_matrix, method = "number")

# Perform correlation test between two attributes
correlation_test <- cor.test(result$TSK, result$TSLB)

# Print the results
print(correlation_test)

# Perform correlation test between two attributes
correlation_test <- cor.test(result$U10, result$SMOIS)

# Print the results
print(correlation_test)

new_result <- result

# U10 and V10 are the X and Y components of wind at 10m respectively
U10 <- new_result$U10
V10 <- new_result$V10

# Calculate wind speed using Pythagorean theorem
wind_speed <- sqrt(U10^2 + V10^2)

# Add the calculated wind speed as a new column to the dataframe
new_result$wind_speed <- wind_speed

# Print the updated dataframe
View(new_result)

#Univariate Analysis

# Box plot for TSK
ggplot(result, aes(x = "", y = TSK)) +
  geom_boxplot() +
  labs(title = "Box plot of Surface Temperature TSK", x = "", y = "TSK")

# Box plot for PSFC
ggplot(result, aes(x = "", y = PSFC)) +
  geom_boxplot() +
  labs(title = "Box plot of PSFC", x = "", y = "PSFC")

# Box plot for U10
ggplot(result, aes(x = "", y = U10)) +
  geom_boxplot() +
  labs(title = "Box plot of U10", x = "", y = "U10")

# Box plot for V10
ggplot(result, aes(x = "", y = V10)) +
  geom_boxplot() +
  labs(title = "Box plot of V10", x = "", y = "V10")

# Box plot for Q2
ggplot(result, aes(x = "", y = Q2)) +
  geom_boxplot() +
  labs(title = "Box plot of Q2", x = "", y = "Q2")

# Box plot for RAINC
ggplot(result, aes(x = "", y = RAINC)) +
  geom_boxplot() +
  labs(title = "Box plot of RAINC", x = "", y = "RAINC")

# Box plot for RAINNC
ggplot(result, aes(x = "", y = RAINNC)) +
  geom_boxplot() +
  labs(title = "Box plot of RAINNC", x = "", y = "RAINNC")

# Box plot for SNOW
ggplot(result, aes(x = "", y = SNOW)) +
  geom_boxplot() +
  labs(title = "Box plot of SNOW", x = "", y = "SNOW")

# Box plot for TSLB
ggplot(result, aes(x = "", y = TSLB)) +
  geom_boxplot() +
  labs(title = "Box plot of TSLB", x = "", y = "TSLB")

# Box plot for SMOIS
ggplot(result, aes(x = "", y = SMOIS)) +
  geom_boxplot() +
  labs(title = "Box plot of SMOIS", x = "", y = "SMOIS")


# Univariate Analysis - Histogram for each variable
ggplot(data = result, aes(x = TSK)) +
  geom_histogram(fill = "blue", bins = 20) +
  labs(title = "Histogram of Surface temperature TSK", x = "TSK", y = "Frequency")

ggplot(data = result, aes(x = PSFC)) +
  geom_histogram(fill = "red", bins = 20) +
  labs(title = "Histogram of Surface pressure PSFC", x = "PSFC", y = "Frequency")

ggplot(data = result, aes(x = U10)) +
  geom_histogram(fill = "green", bins = 20) +
  labs(title = "Histogram of X component of wind U10", x = "U10", y = "Frequency")

ggplot(data = result, aes(x = V10)) +
  geom_histogram(fill = "purple", bins = 20) +
  labs(title = "Histogram of Y component of wind V10", x = "V10", y = "Frequency")

ggplot(data = result, aes(x = Q2)) +
  geom_histogram(fill = "orange", bins = 20) +
  labs(title = "Histogram of 2- meter specific humidity Q2", x = "Q2", y = "Frequency")

ggplot(data = result, aes(x = RAINC)) +
  geom_histogram(fill = "skyblue", bins = 20) +
  labs(title = "Histogram of Consecutive Rain (RAINC)", x = "RAINC", y = "Frequency")

ggplot(data = result, aes(x = RAINNC)) +
  geom_histogram(fill = "skyblue", bins = 20) +
  labs(title = "Histogram Non-Consecutive Rain (RAINNC)", x = "RAINNC", y = "Frequency")

ggplot(data = result, aes(x = TSLB)) +
  geom_histogram(fill = "brown", bins = 20) +
  labs(title = "Histogram Soil temperature TSLB", x = "TSLB", y = "Frequency")

ggplot(data = result, aes(x = SMOIS)) +
  geom_histogram(fill = "maroon", bins = 20) +
  labs(title = "Histogram Soil moisture SMOIS", x = "SMOIS", y = "Frequency")

ggplot(data = new_result, aes(x = wind_speed)) +
  geom_histogram(fill = "yellow", bins = 20) +
  labs(title = "Histogram Wind Speed", x = "Wind Speed", y = "Frequency")


# Function to perform the Shapiro-Wilk test for normality on each column
shapiro_wilk_test <- function(data) {
  results <- list()
  for (column in names(data)) {
    if (is.numeric(data[[column]]) && !is.null(data[[column]]) && !all(duplicated(data[[column]]))) {
      test_result <- tryCatch(shapiro.test(data[[column]]), error = function(e) NULL)
      if (!is.null(test_result)) {
        results[[column]] <- c('Statistic' = test_result$statistic, 'p-value' = test_result$p.value)
      }
    }
  }
  return(results)
}

# Drop the date_time column and perform the Shapiro-Wilk test on the remaining columns
data <- subset(result, select = -c(date_time))  # Drop the date_time column
shapiro_results <- shapiro_wilk_test(data)
shapiro_results

#TSK, PSFC, TSLB, and SMOIS, the p-values are very small (close to 0), indicating strong evidence against the null hypothesis of normality. Therefore, these columns are likely not normally distributed.
#U10 and V10, the p-values are small, but not as small as the previous columns, suggesting less strong evidence against normality.
#Q2, RAINC, and RAINNC, the p-values are relatively large (above 0.05), indicating that we do not have enough evidence to reject the null hypothesis of normality for these columns.



# Perform ANOVA test for each column in the dataset
anova_results <- lapply(result[-1], function(col) {
  if(is.numeric(col)) {
    aov_result <- aov(col ~ 1, data = result)
    return(summary(aov_result))
  } else {
    return(NULL)  # Skip non-numeric columns
  }
})

# Print ANOVA results
print(anova_results)

anova_result <- aov(TSK ~ PSFC, result)
summary(anova_result)

#Bivariate analysis to check hypothesis between surface temperature and convective rain

# Perform correlation analysis
correlation <- cor(result$TSK, result$RAINC)

# Check the correlation coefficient
print(correlation)

# Perform hypothesis testing
# Set significance level (alpha)
alpha <- 0.05

# Perform hypothesis test
if (correlation == 0) {
  cat("There is no correlation between surface temperature and convective rainfall in Glasgow.")
} else {
  p_value <- cor.test(result$TSK, result$RAINC)$p.value
  if (p_value < alpha) {
    cat("Reject the null hypothesis (H0). There is a significant association between surface temperature and convective rainfall in Glasgow.")
  } else {
    cat("Fail to reject the null hypothesis (H0). There is no significant relationship between surface temperature and convective rainfall in Glasgow.")
  }
}

#Bi-Variate analysis based on Annova test

significant_vars <- c("PSFC", "U10", "V10", "TSLB", "SMOIS")

# Scatter plots
pairs(result[, significant_vars], main = "Scatterplot Matrix")

# Correlation matrix
correlation_matrix <- cor(result[, significant_vars])
print(correlation_matrix)

# Visualize correlation matrix
corrplot(correlation_matrix, method = "circle")

# Scatter plots of significant variables against TSK
par(mfrow = c(2, 3)) # Set up the layout for multiple plots
for (var in significant_vars) {
  plot(result[[var]], result$TSK, main = paste("Scatterplot of", var, "vs. TSK"), xlab = var, ylab = "TSK")
}

#Multivariate analysis based on Annova Test

# Fit a multiple linear regression model using significant variables
multivariate_model <- lm(TSK ~ PSFC + U10 + V10 + TSLB + SMOIS, data = result)

# Summary of the multivariate model
summary(multivariate_model)



#Machine Learning models based on Annova Test

# Set seed for reproducibility
set.seed(123)

# Train-test split
train_indices <- sample(1:nrow(result), 0.8 * nrow(result))
train_data <- result[train_indices, ]
test_data <- result[-train_indices, ]

# Linear Regression
linear_model <- lm(TSK ~ PSFC + U10 + V10 + TSLB + SMOIS, data = train_data)

# Random Forest Regression
random_forest_model <- randomForest(TSK ~ PSFC + U10 + V10 + TSLB + SMOIS, data = train_data)

# Decision Tree Regression
decision_tree_model <- rpart(TSK ~ PSFC + U10 + V10 + TSLB + SMOIS, data = train_data)

# Evaluate Models

# for the linear model:
predictions_linear <- predict(linear_model, newdata = test_data)
mse_linear <- mean((test_data$TSK - predictions_linear)^2)
mae_linear <- mean(abs(test_data$TSK - predictions_linear))
rsquared_linear <- cor(test_data$TSK, predictions_linear)^2

paste("MSE Linear", mse_linear, "MAE LINEAR", mae_linear, "RSQUARED Linear", rsquared_linear)

# for the Random Forest model:
predictions_random <- predict(random_forest_model, newdata = test_data)
mse_random <- mean((test_data$TSK - predictions_random)^2)
mae_random <- mean(abs(test_data$TSK - predictions_random))
rsquared_random <- cor(test_data$TSK, predictions_random)^2

paste("MSE Random", mse_random, "MAE Random", mae_random, "RSQUARED Random", rsquared_random)


# for the Decision Tree model:
predictions_decision <- predict(decision_tree_model, newdata = test_data)
mse_decision <- mean((test_data$TSK - predictions_decision)^2)
mae_decision <- mean(abs(test_data$TSK - predictions_decision))
rsquared_decision <- cor(test_data$TSK, predictions_decision)^2

paste("MSE Decision", mse_decision, "MAE Decision", mae_decision, "RSQUARED Decision", rsquared_decision)

summary(linear_model)

#Timeseries Analysis

# Load necessary libraries
library(forecast)
library(tseries)

# Convert 'date_time' column to POSIXct format
result$date_time <- as.POSIXct(result$date_time)

# Create a time series object with the correct frequency and start date
ts_data <- ts(result$TSK, frequency = 8)

print(ts_data)
class(ts_data)
time(ts_data)


plot(result$date_time, ts_data, type = "l", main = "Time Series Plot of TSK", xlab = "Date and Time", ylab = "TSK")

# Decompose Time Series
decomp <- decompose(ts_data)
plot(decomp)

# Check autocorrelation function (ACF) plot to detect seasonality
acf(ts_data)

# You can also use seasonal plots to visualize seasonality
seasonplot(ts_data)

ljung_box_test <- Box.test(residuals(lm(ts_data ~ 1)), lag = 6, type = "Ljung-Box")

# Print the p-value of the test
print(ljung_box_test$p.value)

# Check if the p-value is less than a significance level (e.g., 0.05) to determine if there's significant autocorrelation
if (ljung_box_test$p.value < 0.05) {
  print("The data exhibits significant autocorrelation, indicating potential seasonality.")
} else {
  print("The data does not exhibit significant autocorrelation, suggesting no clear seasonality.")
}

# Apply Augmented Dickey-Fuller (ADF) test
adf_test <- adf.test(ts_data)
print(adf_test)

# Fit ARIMA model to training data
arima_model <- auto.arima(ts_data)

# Validate the model
checkresiduals(arima_model)


# Forecast future values
forecast_values <- forecast(arima_model, h = 10)

# Print forecasted values
print(forecast_values)
plot(forecast_values)

#Manual Timeseries
autoplot(ts_data)
ggseasonplot(ts_data)

tsdisplay(ts_data)
tail(ts_data)

acf_plot <- acf(ts_data)
pacf_plot <- pacf(ts_data)

myarima<- Arima(ts_data, order = c(1,0,1), seasonal=list(order=c(1,1,1)))
myarima
checkresiduals(myarima)

myarima<- Arima(ts_data, order = c(1,0,2), seasonal=list(order=c(1,1,1)))
myarima
checkresiduals(myarima)

myarima<- Arima(ts_data, order = c(1,0,3), seasonal=list(order=c(1,1,1)))
myarima
checkresiduals(myarima)

myarima<- Arima(ts_data, order = c(6,0,1), seasonal=list(order=c(1,1,1)))
myarima
checkresiduals(myarima)

myarima<- Arima(ts_data, order = c(6,0,2), seasonal=list(order=c(1,1,1)))
myarima
checkresiduals(myarima)

#Fit the SARIMA Model(p=1, d=0, q=2) (P=1, D=1, Q=1, s=12)
sarima_model <- arima(ts_data, order = c(1,0,2), seasonal=list(order=c(1,1,1)))

#Forecast the next 48 periods
forecast_values_manual <- forecast(sarima_model, h=48)
plot(forecast_values_manual, type = "l", main = "Surface Temperature Forecast(1,0,2)", xlab = "Date", ylab = "Temperature(K)")



